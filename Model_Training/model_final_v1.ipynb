{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageDataGenerator\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Define image size\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ArushWindows\\.conda\\envs\\tensorflow_venv\\Lib\\site-packages\\pandas\\__init__.py:27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _dependency \u001b[38;5;129;01min\u001b[39;00m _hard_dependencies:\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 27\u001b[0m         \u001b[38;5;28;43m__import__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_dependency\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m _e:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[0;32m     29\u001b[0m         _missing_dependencies\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_dependency\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_e\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ArushWindows\\.conda\\envs\\tensorflow_venv\\Lib\\site-packages\\numpy\\__init__.py:171\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fft\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m polynomial\n\u001b[1;32m--> 171\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m random\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ctypeslib\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ma\n",
      "File \u001b[1;32mc:\\Users\\ArushWindows\\.conda\\envs\\tensorflow_venv\\Lib\\site-packages\\numpy\\random\\__init__.py:180\u001b[0m\n\u001b[0;32m    126\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeta\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinomial\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzipf\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    177\u001b[0m ]\n\u001b[0;32m    179\u001b[0m \u001b[38;5;66;03m# add these for module-freeze analysis (like PyInstaller)\u001b[39;00m\n\u001b[1;32m--> 180\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _pickle\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _common\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _bounded_integers\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1147\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:690\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:936\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1032\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1130\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define image size\n",
    "img_size = (469, 369)\n",
    "\n",
    "# Load CSV files\n",
    "df_train = pd.read_csv(\n",
    "    \"/content/Continous_Check/cont_data_train.csv\"\n",
    ")\n",
    "df_test = pd.read_csv(\n",
    "    \"/content/Continous_Check/cont_data_test.csv\"\n",
    ")\n",
    "df_val = pd.read_csv(\n",
    "    \"/content/Continous_Check/cont_data_val.csv\"\n",
    ")\n",
    "\n",
    "# Data generator for scaling images\n",
    "scale = ImageDataGenerator(rescale=1.0 / 255)\n",
    "\n",
    "# Create train, test, and validation generators\n",
    "train_generator = scale.flow_from_dataframe(\n",
    "    df_train,\n",
    "    x_col=\"Path\",\n",
    "    y_col=[\"Label\",\"ChirpMass\"],\n",
    "    target_size=img_size,\n",
    "    class_mode=\"raw\",  # For regression, use 'raw'\n",
    "    batch_size=32,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_generator = scale.flow_from_dataframe(\n",
    "    df_test,\n",
    "    x_col=\"Path\",\n",
    "    y_col=[\"Label\",\"ChirpMass\"],\n",
    "    target_size=img_size,\n",
    "    class_mode=\"raw\",  # For regression, use 'raw'\n",
    "    batch_size=32,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_generator = scale.flow_from_dataframe(\n",
    "    df_val,\n",
    "    x_col=\"Path\",\n",
    "    y_col=[\"Label\",\"ChirpMass\"],\n",
    "    target_size=img_size,\n",
    "    class_mode=\"raw\",  # For regression, use 'raw'\n",
    "    batch_size=32,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def create_cnn_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # Convolutional layers\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Flatten the output of the convolutional layers\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    # Fully connected layers\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    # Output layers for different parameters\n",
    "    t_merger_output = Dense(1, name='t_merger', activation='linear')(x)\n",
    "    chirp_mass_output = Dense(1, name='chirp_mass', activation='linear')(x)\n",
    "    SNR = Dense(1,name='SNR',activation='linear')(x)\n",
    "\n",
    "    # Define the model with the inputs and outputs\n",
    "    model = Model(inputs=inputs, outputs=[t_merger_output, chirp_mass_output, SNR])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def chirp_mass_loss(y_true, y_pred):\n",
    "\n",
    "    # Calculate the mean squared error\n",
    "    mse = K.mean(K.square(y_pred - y_true), axis=-1)\n",
    "\n",
    "    # Scale the MSE by (chirp mass)^(5/3)\n",
    "    chirp_mass_factor = K.pow(y_true, 5/3)\n",
    "\n",
    "    return mse * chirp_mass_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)        [(None, 469, 369, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)           (None, 469, 369, 32)         896       ['input_3[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling2d_6 (MaxPoolin  (None, 234, 184, 32)         0         ['conv2d_6[0][0]']            \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " batch_normalization_6 (Bat  (None, 234, 184, 32)         128       ['max_pooling2d_6[0][0]']     \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)           (None, 234, 184, 64)         18496     ['batch_normalization_6[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " max_pooling2d_7 (MaxPoolin  (None, 117, 92, 64)          0         ['conv2d_7[0][0]']            \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " batch_normalization_7 (Bat  (None, 117, 92, 64)          256       ['max_pooling2d_7[0][0]']     \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)           (None, 117, 92, 128)         73856     ['batch_normalization_7[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " max_pooling2d_8 (MaxPoolin  (None, 58, 46, 128)          0         ['conv2d_8[0][0]']            \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " batch_normalization_8 (Bat  (None, 58, 46, 128)          512       ['max_pooling2d_8[0][0]']     \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)         (None, 341504)               0         ['batch_normalization_8[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dense_4 (Dense)             (None, 512)                  1748505   ['flatten_2[0][0]']           \n",
      "                                                          60                                      \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)         (None, 512)                  0         ['dense_4[0][0]']             \n",
      "                                                                                                  \n",
      " dense_5 (Dense)             (None, 256)                  131328    ['dropout_4[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)         (None, 256)                  0         ['dense_5[0][0]']             \n",
      "                                                                                                  \n",
      " t_merger (Dense)            (None, 1)                    257       ['dropout_5[0][0]']           \n",
      "                                                                                                  \n",
      " chirp_mass (Dense)          (None, 1)                    257       ['dropout_5[0][0]']           \n",
      "                                                                                                  \n",
      " SNR (Dense)                 (None, 1)                    257       ['dropout_5[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 175076803 (667.87 MB)\n",
      "Trainable params: 175076355 (667.86 MB)\n",
      "Non-trainable params: 448 (1.75 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_shape = (469, 369, 3)\n",
    "model = create_cnn_model(input_shape)\n",
    "\n",
    "# Compile the model with appropriate loss functions and optimizer\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4),\n",
    "              loss={'t_merger': 'mse',\n",
    "                    'sky_location': 'mse',\n",
    "                    'distance': 'mse',\n",
    "                    'chirp_mass': chirp_mass_loss},\n",
    "                    \n",
    "              metrics={'t_merger': 'mae',\n",
    "                       'sky_location': 'mae',\n",
    "                       'distance': 'mae',\n",
    "                       'chirp_mass': 'mae'})\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39_v1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
